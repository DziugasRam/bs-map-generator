{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import random\n",
    "from data_loader import full_load_map, data_dir, load_map, encode_empty, encode_to_array, Note\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import json\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 6969\n",
    "\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator_single_process(map_folders):\n",
    "    for map_folder in map_folders:\n",
    "        try:\n",
    "            results = full_load_map(map_folder.decode('UTF-8'))\n",
    "            for result in results:\n",
    "                x_context_prev_audio, x_context_prev_notes, x_context_audio, y_context_notes = result\n",
    "                yield np.array(x_context_prev_audio), np.array(x_context_prev_notes), np.array(x_context_audio), np.sum(np.max(y_context_notes, axis=2), axis=1, keepdims=True)/25, np.sum(np.sum(y_context_notes, axis=2), axis=1, keepdims=True)/25, np.array(y_context_notes)\n",
    "        except Exception as exc:\n",
    "            if str(exc) != \"'_version'\" and str(exc) != 'not v2':\n",
    "                    print(exc)\n",
    "\n",
    "def data_generator_multi_process(map_folders):\n",
    "    max_workers = 8\n",
    "    items_in_queue = max_workers * 2\n",
    "    queued_maps = items_in_queue\n",
    "    cancel = False\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        map_tasks = [executor.submit(full_load_map, map_folder) for map_folder in map_folders[:items_in_queue]]\n",
    "        while len(map_tasks) > 0:\n",
    "            map_task = map_tasks.pop(0)\n",
    "            try:\n",
    "                if cancel:\n",
    "                    map_task.cancel()\n",
    "                    continue\n",
    "                results = map_task.result()\n",
    "                for result in results:\n",
    "                    x_context_prev_audio, x_context_prev_notes, x_context_audio, y_context_notes = result\n",
    "                    x_context_prev_audio = np.array(x_context_prev_audio, dtype=np.float32)\n",
    "                    x_context_prev_notes = np.array(x_context_prev_notes, dtype=np.float32)\n",
    "                    x_context_audio = np.array(x_context_audio, dtype=np.float32)\n",
    "                    y_context_notes = np.array(y_context_notes, dtype=np.float32)\n",
    "                    yield (x_context_prev_audio), (x_context_prev_notes), (x_context_audio), np.sum(np.max(y_context_notes, axis=2), axis=1, keepdims=True)/25, np.sum(np.sum(y_context_notes, axis=2), axis=1, keepdims=True)/25, (y_context_notes)\n",
    "            except InterruptedError as ke:\n",
    "                cancel = True\n",
    "            except Exception as exc:\n",
    "                if str(exc) != \"'_version'\" and str(exc) != 'not v2':\n",
    "                    print(exc)\n",
    "            finally:\n",
    "                if not cancel:\n",
    "                    queued_maps += 1\n",
    "                    if queued_maps < len(map_folders):\n",
    "                        map_tasks.append(executor.submit(full_load_map, map_folders[queued_maps]))\n",
    "                \n",
    "# def data_generator(map_folders):\n",
    "#     try:\n",
    "#         with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "#             results = executor.map(load_map_asd, map_folders)\n",
    "\n",
    "#             for result in results:\n",
    "#                 print(result)\n",
    "#                 if result is not None:\n",
    "#                     yield result\n",
    "#     except Exception as e:\n",
    "#         traceback.print_exc()\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ds_for_files(map_folders, batch_size, cache=False, shuffle=False):\n",
    "    ds = tf.data.Dataset.from_generator(data_generator_multi_process, args=[map_folders], output_signature=(\n",
    "        tf.TensorSpec(shape=(None, 87, 129), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 50, 217), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 87, 129), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 50, 217), dtype=tf.float32),\n",
    "        # tf.TensorSpec(shape=(None, 1025, 44), dtype=tf.float32),\n",
    "        # tf.TensorSpec(shape=(None, 35), dtype=tf.float32),\n",
    "    ))\n",
    "    ds = ds.flat_map(lambda x1, x2, x3, x4, x5, y: tf.data.Dataset.from_tensor_slices((x1, x2, x3, x4, x5, y)))\n",
    "\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(5000, reshuffle_each_iteration=True)\n",
    "        # ds = ds.shuffle(len([v for v in ds]), reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(256)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I currently cache the entire dataset, since the data loading part is quite compute intensive. Added a limit of 50 maps to avoid running out of ram on a test run.\n",
    "maps = [path.replace(\"\\\\\", \"/\") for path in glob.glob(f\"{data_dir}/*\")][:50]\n",
    "random.shuffle(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_ds = create_ds_for_files(maps, batch_size, True, True)\n",
    "# val_ds = create_ds_for_files(maps[:50], batch_size, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_block():\n",
    "    input_audio = tf.keras.Input(shape=(87, 129, 1), dtype=\"float32\")\n",
    "    l = input_audio\n",
    "    l = tf.keras.layers.Conv2D(32, 3, activation=\"relu\", padding='same')(l)\n",
    "    l = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same')(l)\n",
    "    l = tf.keras.layers.Conv2D(32, 3, activation=\"relu\", padding='same')(l)\n",
    "    l = tf.keras.layers.Reshape((44, -1))(l)\n",
    "    l = tf.keras.layers.ZeroPadding1D(3)(l)\n",
    "    l = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(l)\n",
    "    l = tf.keras.layers.LSTM(64, return_sequences=True)(l)\n",
    "    return tf.keras.Model(input_audio, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    input_prev_audio = tf.keras.Input(shape=(87, 129, 1), dtype=\"float32\")\n",
    "    input_prev_notes = tf.keras.Input(shape=(50, 217), dtype=\"float32\")\n",
    "    input_audio = tf.keras.Input(shape=(87, 129, 1), dtype=\"float32\")\n",
    "    input_intensity_1 = tf.keras.Input(shape=(1), dtype=\"float32\")\n",
    "    input_intensity_2 = tf.keras.Input(shape=(1), dtype=\"float32\")\n",
    "\n",
    "    audio_l = audio_block()\n",
    "\n",
    "    l_prev_audio = audio_l(input_prev_audio)\n",
    "    l_audio = audio_l(input_audio)\n",
    "    \n",
    "    l_prev = tf.keras.layers.Concatenate(axis=2)([l_prev_audio, input_prev_notes])\n",
    "    l_prev = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True))(l_prev)\n",
    "    l_prev = tf.keras.layers.LSTM(32)(l_prev)\n",
    "    \n",
    "    \n",
    "    l_intensity_1 = tf.keras.layers.RepeatVector(50)(input_intensity_1)\n",
    "    l_intensity_2 = tf.keras.layers.RepeatVector(50)(input_intensity_2)\n",
    "    l_prev = tf.keras.layers.RepeatVector(50)(l_prev)\n",
    "    l = tf.keras.layers.Concatenate(axis=2)([l_audio, l_prev, l_intensity_1, l_intensity_1, l_intensity_1, l_intensity_2, l_intensity_2, l_intensity_2])\n",
    "    l = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(l)\n",
    "    l = tf.keras.layers.LSTM(128, return_sequences=True)(l)\n",
    "    l = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(217, activation=\"sigmoid\"))(l)\n",
    "    output = l\n",
    "\n",
    "    model = tf.keras.Model(inputs = [input_prev_audio, input_prev_notes, input_audio, input_intensity_1, input_intensity_2], outputs = output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 87, 129, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 87, 129, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " model (Functional)             (None, 50, 64)       1157216     ['input_1[0][0]',                \n",
      "                                                                  'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 50, 217)]    0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 50, 281)      0           ['model[0][0]',                  \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 50, 64)      80384       ['concatenate[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  (None, 32)           12416       ['bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " repeat_vector_2 (RepeatVector)  (None, 50, 32)      0           ['lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      " repeat_vector (RepeatVector)   (None, 50, 1)        0           ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " repeat_vector_1 (RepeatVector)  (None, 50, 1)       0           ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 50, 102)      0           ['model[1][0]',                  \n",
      "                                                                  'repeat_vector_2[0][0]',        \n",
      "                                                                  'repeat_vector[0][0]',          \n",
      "                                                                  'repeat_vector[0][0]',          \n",
      "                                                                  'repeat_vector[0][0]',          \n",
      "                                                                  'repeat_vector_1[0][0]',        \n",
      "                                                                  'repeat_vector_1[0][0]',        \n",
      "                                                                  'repeat_vector_1[0][0]']        \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, 50, 256)     236544      ['concatenate_1[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)                  (None, 50, 128)      197120      ['bidirectional_2[0][0]']        \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 50, 217)     27993       ['lstm_5[0][0]']                 \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,711,673\n",
      "Trainable params: 1,711,673\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_timing_loss_metric = tf.keras.metrics.Mean(name='train_timing_loss')\n",
    "train_positioning_loss_metric = tf.keras.metrics.Mean(name='train_positioning_loss')\n",
    "train_loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "val_timing_loss_metric = tf.keras.metrics.Mean(name='val_timing_loss')\n",
    "val_positioning_loss_metric = tf.keras.metrics.Mean(name='val_positioning_loss')\n",
    "val_loss_metric = tf.keras.metrics.Mean(name='val_loss')\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1979it [00:05, 358.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 517127.0\n",
      "0 4694.0\n",
      "1 23756.0\n",
      "2 17569.0\n",
      "3 8446.0\n",
      "4 2427.0\n",
      "5 26532.0\n",
      "6 3123.0\n",
      "7 536.0\n",
      "8 5016.0\n",
      "9 60.0\n",
      "10 32.0\n",
      "11 15.0\n",
      "12 21657.0\n",
      "13 8832.0\n",
      "14 765.0\n",
      "15 51694.0\n",
      "16 1196.0\n",
      "17 800.0\n",
      "18 12045.0\n",
      "19 543.0\n",
      "20 398.0\n",
      "21 148.0\n",
      "22 39.0\n",
      "23 2.0\n",
      "24 1076.0\n",
      "25 4227.0\n",
      "26 224.0\n",
      "27 430.0\n",
      "28 209.0\n",
      "29 109.0\n",
      "30 76.0\n",
      "31 13.0\n",
      "32 17.0\n",
      "33 17.0\n",
      "34 14.0\n",
      "35 5.0\n",
      "36 83.0\n",
      "37 95.0\n",
      "38 18.0\n",
      "39 509.0\n",
      "40 31.0\n",
      "41 61.0\n",
      "42 1605.0\n",
      "43 228.0\n",
      "44 211.0\n",
      "45 690.0\n",
      "46 457.0\n",
      "47 52.0\n",
      "48 847.0\n",
      "49 24681.0\n",
      "50 9004.0\n",
      "51 2701.0\n",
      "52 658.0\n",
      "53 1645.0\n",
      "54 542.0\n",
      "55 72.0\n",
      "56 146.0\n",
      "57 23.0\n",
      "58 47.0\n",
      "59 6.0\n",
      "60 99.0\n",
      "61 332.0\n",
      "62 64.0\n",
      "63 77.0\n",
      "64 62.0\n",
      "65 815.0\n",
      "66 73.0\n",
      "67 49.0\n",
      "68 1109.0\n",
      "69 4.0\n",
      "70 51.0\n",
      "71 38.0\n",
      "72 2106.0\n",
      "73 3799.0\n",
      "74 131.0\n",
      "75 261.0\n",
      "76 91.0\n",
      "77 216.0\n",
      "78 33.0\n",
      "79 30.0\n",
      "80 46.0\n",
      "81 0.0\n",
      "82 14.0\n",
      "83 0.0\n",
      "84 283.0\n",
      "85 2172.0\n",
      "86 182.0\n",
      "87 9002.0\n",
      "88 450.0\n",
      "89 634.0\n",
      "90 22513.0\n",
      "91 418.0\n",
      "92 441.0\n",
      "93 1096.0\n",
      "94 413.0\n",
      "95 6.0\n",
      "96 2661.0\n",
      "97 4900.0\n",
      "98 3713.0\n",
      "99 4922.0\n",
      "100 543.0\n",
      "101 2512.0\n",
      "102 2791.0\n",
      "103 167.0\n",
      "104 520.0\n",
      "105 266.0\n",
      "106 147.0\n",
      "107 15.0\n",
      "108 112.0\n",
      "109 57.0\n",
      "110 32.0\n",
      "111 3307.0\n",
      "112 527.0\n",
      "113 5064.0\n",
      "114 8735.0\n",
      "115 2358.0\n",
      "116 25985.0\n",
      "117 4806.0\n",
      "118 25191.0\n",
      "119 17676.0\n",
      "120 250.0\n",
      "121 90.0\n",
      "122 5.0\n",
      "123 12177.0\n",
      "124 617.0\n",
      "125 425.0\n",
      "126 52476.0\n",
      "127 1132.0\n",
      "128 837.0\n",
      "129 22356.0\n",
      "130 9003.0\n",
      "131 716.0\n",
      "132 756.0\n",
      "133 685.0\n",
      "134 63.0\n",
      "135 1865.0\n",
      "136 312.0\n",
      "137 239.0\n",
      "138 613.0\n",
      "139 69.0\n",
      "140 96.0\n",
      "141 85.0\n",
      "142 99.0\n",
      "143 22.0\n",
      "144 15.0\n",
      "145 14.0\n",
      "146 7.0\n",
      "147 118.0\n",
      "148 25.0\n",
      "149 21.0\n",
      "150 495.0\n",
      "151 262.0\n",
      "152 149.0\n",
      "153 1153.0\n",
      "154 4687.0\n",
      "155 296.0\n",
      "156 7.0\n",
      "157 69.0\n",
      "158 51.0\n",
      "159 63.0\n",
      "160 59.0\n",
      "161 1076.0\n",
      "162 67.0\n",
      "163 64.0\n",
      "164 780.0\n",
      "165 99.0\n",
      "166 365.0\n",
      "167 67.0\n",
      "168 27.0\n",
      "169 73.0\n",
      "170 9.0\n",
      "171 630.0\n",
      "172 95.0\n",
      "173 157.0\n",
      "174 3162.0\n",
      "175 726.0\n",
      "176 1936.0\n",
      "177 916.0\n",
      "178 27296.0\n",
      "179 9557.0\n",
      "180 1377.0\n",
      "181 506.0\n",
      "182 11.0\n",
      "183 24400.0\n",
      "184 537.0\n",
      "185 469.0\n",
      "186 9902.0\n",
      "187 527.0\n",
      "188 755.0\n",
      "189 288.0\n",
      "190 2470.0\n",
      "191 206.0\n",
      "192 1.0\n",
      "193 18.0\n",
      "194 0.0\n",
      "195 29.0\n",
      "196 38.0\n",
      "197 62.0\n",
      "198 327.0\n",
      "199 115.0\n",
      "200 299.0\n",
      "201 2220.0\n",
      "202 3943.0\n",
      "203 139.0\n",
      "204 318.0\n",
      "205 181.0\n",
      "206 30.0\n",
      "207 3026.0\n",
      "208 213.0\n",
      "209 620.0\n",
      "210 5222.0\n",
      "211 593.0\n",
      "212 2617.0\n",
      "213 2773.0\n",
      "214 5244.0\n",
      "215 3999.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 216), dtype=float32, numpy=\n",
       "array([[[1.        , 0.24908063, 0.33679548, 0.7005872 , 1.        ,\n",
       "         0.22301973, 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 0.27322155, 0.66996825, 1.        ,\n",
       "         0.11446512, 1.        , 1.        , 0.4912544 , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 0.23974553,\n",
       "         0.6571701 , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 0.6573161 , 1.        , 1.        ,\n",
       "         0.26283303, 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 0.6774081 ,\n",
       "         1.        , 0.22771443, 1.        , 0.2348918 , 0.3347567 ,\n",
       "         1.        , 1.        , 1.        , 0.48592916, 1.        ,\n",
       "         1.        , 0.11275934, 1.        , 1.        , 0.2646788 ,\n",
       "         0.65724313, 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 0.21677753, 0.619144  ,\n",
       "         1.        , 1.        , 1.        , 0.24250655, 1.        ,\n",
       "         1.        , 0.59757215, 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        ]]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_value_counts = tf.constant([0]*217, dtype=tf.float32)\n",
    "\n",
    "for v_batch in tqdm(train_ds):\n",
    "    out_value_counts = out_value_counts + tf.reduce_sum(v_batch[-1], axis=[0, 1])\n",
    "# for i, ovc in enumerate(out_value_counts):\n",
    "#     print(f\"{(i-1)} {ovc}\")\n",
    "note_poss_loss_balance = tf.expand_dims(tf.expand_dims(1/tf.maximum(out_value_counts[1:] * 0.000169, 1), 0), 0)\n",
    "note_poss_loss_balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom loss\n",
    "Calculates 2 separate values:\n",
    " - timing loss - simple loss based on when the notes were placed\n",
    " - positioning loss - loss based on the position and direction of the placed note, adjusted for the number of notes that appear in different positions to avoid a massive bias towards placing most commonly appearing notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def custom_loss(y, predictions):\n",
    "    loss_matrix = tf.square(tf.abs(y - predictions))\n",
    "    timing_loss_matrix = loss_matrix[:, :, 0]\n",
    "    positioning_loss_matrix = loss_matrix[:, :, 1:] * y[:, :, :1] * (y[:, :, 1:] * note_poss_loss_balance + 0.0169)\n",
    "    timing_loss = tf.reduce_mean(timing_loss_matrix)\n",
    "    positioning_loss = tf.reduce_sum(positioning_loss_matrix) / tf.reduce_sum(y[:, :, 1:]) * 0.5\n",
    "    loss = timing_loss + positioning_loss\n",
    "    return timing_loss, positioning_loss, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, optimizer, data):\n",
    "    x1, x2, x3, x4, x5, y = data\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model([x1, x2, x3, x4, x5], training=True)\n",
    "        with tf.device('/CPU:0'):\n",
    "            timing_loss, positioning_loss, loss = custom_loss(y, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_timing_loss_metric(timing_loss)\n",
    "    train_positioning_loss_metric(positioning_loss)\n",
    "    train_loss_metric(loss)\n",
    "\n",
    "@tf.function\n",
    "def val_step(model, data):\n",
    "    x1, x2, x3, x4, x5, y = data\n",
    "    \n",
    "    predictions = model([x1, x2, x3, x4, x5], training=False)\n",
    "    timing_loss, positioning_loss, loss = custom_loss(y, predictions)\n",
    "        \n",
    "    val_timing_loss_metric(timing_loss)\n",
    "    val_positioning_loss_metric(positioning_loss)\n",
    "    val_loss_metric(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch train: 0: : 1979batch [02:16, 14.51batch/s, loss=0.174, positioning_loss=0.135, timing_loss=0.0385]\n",
      "Epoch train: 1: : 1979batch [02:15, 14.64batch/s, loss=0.17, positioning_loss=0.133, timing_loss=0.0369]\n",
      "Epoch train: 2: : 1979batch [02:15, 14.64batch/s, loss=0.166, positioning_loss=0.13, timing_loss=0.0355] \n",
      "Epoch train: 3: : 1979batch [02:35, 12.75batch/s, loss=0.161, positioning_loss=0.127, timing_loss=0.034] \n",
      "Epoch train: 4: : 1979batch [02:29, 13.21batch/s, loss=0.157, positioning_loss=0.124, timing_loss=0.0326]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, 5):\n",
    "    train_timing_loss_metric.reset_states()\n",
    "    train_positioning_loss_metric.reset_states()\n",
    "    train_loss_metric.reset_states()\n",
    "    \n",
    "    val_timing_loss_metric.reset_states()\n",
    "    val_positioning_loss_metric.reset_states()\n",
    "    val_loss_metric.reset_states()\n",
    "\n",
    "    \n",
    "    with tqdm(train_ds.enumerate(), unit=\"batch\") as _tqdm:\n",
    "        _tqdm.set_description(f\"Epoch train: {epoch}\")\n",
    "        for step, data in _tqdm:\n",
    "            train_step(model, optimizer, data)\n",
    "            _tqdm.set_postfix(\n",
    "                timing_loss=train_timing_loss_metric.result().numpy(),\n",
    "                positioning_loss=train_positioning_loss_metric.result().numpy(),\n",
    "                loss=train_loss_metric.result().numpy(),\n",
    "            )\n",
    "    \n",
    "    if 'val_ds' in locals() or 'val_ds' in globals():\n",
    "        with tqdm(val_ds.enumerate(), unit=\"batch\") as _tqdm:\n",
    "            _tqdm.set_description(f\"Epoch val: {epoch}\")\n",
    "            for step, data in _tqdm:\n",
    "                val_step(model, data)\n",
    "                _tqdm.set_postfix(\n",
    "                    timing_loss=val_timing_loss_metric.result().numpy(),\n",
    "                    positioning_loss=val_positioning_loss_metric.result().numpy(),\n",
    "                    loss=val_loss_metric.result().numpy(),\n",
    "                )#2302 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haven't yet reached results worth saving :D\n",
    "# model.save(\"./models/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results validation\n",
    "\n",
    "- specify the correct folder with maps for which you would want to generate the map\n",
    "- add maps that you want to use for testing, better to avoid using the maps that already exist in the training dataset to avoid false positives of AI learning a specific map\n",
    "- add an Expert diff if doesn't exist, currently hardcoded to just override the Expert diff to avoid setting up all the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_validation_path = \"./validation\"\n",
    "os.makedirs(base_validation_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_map = \"lifesux\"\n",
    "(song_data, segment_duration), diffs = load_map(base_validation_path + validation_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on previous note format, ignore\n",
    "def validate_model_old(song_data, segment_duration, threshhold, diff):\n",
    "    context_length = 1\n",
    "    note_time_delta = 0.05\n",
    "    note_count = 10\n",
    "    note_length = 35\n",
    "\n",
    "    note_iterator = 0\n",
    "\n",
    "    context_steps = int(context_length / segment_duration) + 1\n",
    "    step_size = int(note_time_delta / segment_duration) + 1\n",
    "    \n",
    "    generated_notes = diff.notes[:10]\n",
    "    max_val = 0\n",
    "    for i in tqdm(range(context_steps, song_data.shape[1] - context_steps, step_size)):\n",
    "        curr_time = i * segment_duration\n",
    "        \n",
    "        while len(generated_notes) > note_iterator and generated_notes[note_iterator].time < curr_time:\n",
    "            note_iterator += 1\n",
    "\n",
    "        x_precontext_audio = song_data[:, i-context_steps:i]\n",
    "        x_postcontext_audio = song_data[:, i:i+context_steps]\n",
    "        \n",
    "        notes = []\n",
    "        for _j in range(note_count):\n",
    "            j = 10 - _j\n",
    "            curr_iter = note_iterator - j\n",
    "            if curr_iter < 0 or generated_notes[curr_iter].time < curr_time - context_length:\n",
    "                encode_empty(notes)\n",
    "            else:\n",
    "                encode_to_array(generated_notes[curr_iter], notes, curr_time)\n",
    "        \n",
    "        prediction = np.array(model([np.array([x_precontext_audio]), np.array([notes]), np.array([x_postcontext_audio])])[0], dtype=np.float32)\n",
    "\n",
    "        if max_val < np.max(prediction[1:]):\n",
    "            max_val = np.max(prediction[1:])\n",
    "            print(max_val)\n",
    "            \n",
    "        \n",
    "        if np.max(prediction[1:]) > threshhold:\n",
    "            print(\"threshhold\")\n",
    "            predicted_note_time = np.minimum(np.maximum(0, prediction[0]), note_time_delta)\n",
    "            type0line = prediction[1:5]\n",
    "            type0layer = prediction[5:8]\n",
    "            type0direction = prediction[8:17]\n",
    "            type0bombdir = prediction[17]\n",
    "            \n",
    "            type1line = prediction[18:22]\n",
    "            type1layer = prediction[22:25]\n",
    "            type1direction = prediction[25:34]\n",
    "            type1bombdir = prediction[34]\n",
    "            \n",
    "            type0sum = np.max(type0line) + np.max(type0layer) + np.max(type0direction)\n",
    "            type1sum = np.max(type1line) + np.max(type1layer) + np.max(type1direction)\n",
    "            # bombtype = \n",
    "            \n",
    "            if type0sum > type1sum:\n",
    "                generated_notes.append(Note(predicted_note_time + curr_time, np.argmax(type0line), np.argmax(type0layer), 0, np.argmax(type0direction)))\n",
    "            else:\n",
    "                generated_notes.append(Note(predicted_note_time + curr_time, np.argmax(type1line), np.argmax(type1layer), 1, np.argmax(type1direction)))\n",
    "            generated_notes.sort(key=lambda note: note.time)\n",
    "            threshhold += 0.015\n",
    "        else:\n",
    "            threshhold -= 0.01\n",
    "    return generated_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/171 [00:00<00:13, 12.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tim: 0.8456912636756897\n",
      "pos: 0.8791457414627075\n",
      "tim: 0.9187914133071899\n",
      "pos: 0.9310385584831238\n",
      "tim: 0.9562196731567383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 6/171 [00:00<00:13, 12.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: 0.9365254044532776\n",
      "tim: 0.9650704860687256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 12/171 [00:00<00:12, 12.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tim: 0.9686254262924194\n",
      "tim: 0.9719038605690002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 20/171 [00:01<00:11, 13.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: 0.9486384987831116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 34/171 [00:02<00:10, 13.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tim: 0.9722039103507996\n",
      "tim: 0.9756019711494446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 38/171 [00:02<00:09, 13.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tim: 0.9804081916809082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 52/171 [00:03<00:08, 13.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tim: 0.9809088706970215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 56/171 [00:04<00:08, 13.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: 0.9522011280059814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 92/171 [00:06<00:05, 13.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tim: 0.9848037958145142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:12<00:00, 13.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5992377264321425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def validate_model(song_data, segment_duration, timing_threshhold, positioning_threshhold, intensity_1, intensity_2):\n",
    "    context_length = 1\n",
    "    prediction_note_count = context_length * 50\n",
    "\n",
    "    context_steps = int(context_length / segment_duration) + 1\n",
    "    step_size = context_steps\n",
    "    \n",
    "    generated_notes = []\n",
    "    max_val_timing = 0\n",
    "    max_val_positioning = 0\n",
    "    \n",
    "    prev_note_segment = [[0]*217 for i in range(prediction_note_count)]\n",
    "    prev_audio_segment = song_data[:context_steps, :]\n",
    "    \n",
    "    for i in tqdm(range(context_steps, song_data.shape[0] - context_steps, step_size)):\n",
    "        curr_time = i * segment_duration\n",
    "        \n",
    "        x_context_prev_audio = prev_audio_segment\n",
    "        x_context_prev_notes = prev_note_segment\n",
    "        x_context_audio = song_data[i:i+context_steps, :]\n",
    "        prediction = np.array(model([np.array([x_context_prev_audio]), np.array([x_context_prev_notes]), np.array([x_context_audio]), np.array([intensity_1]), np.array([intensity_2])], training=False)[0], dtype=np.float32)\n",
    "        x_context_prev_audio = x_context_audio\n",
    "        prev_note_segment = [[0]*217 for i in range(prediction_note_count)]\n",
    "\n",
    "        # I use them to find values that would generate a reasonable number of notes.\n",
    "        # Small adjustments to the model and it's loss function can significantly shift the actual number values.\n",
    "        if max_val_timing < np.max(prediction[:, 0]):\n",
    "            max_val_timing = np.max(prediction[:, 0])\n",
    "            print(f\"max_timing: {max_val_timing}\")\n",
    "        if max_val_positioning < np.max(prediction[:, 1:]):\n",
    "            max_val_positioning = np.max(prediction[:, 1:])\n",
    "            print(f\"max_positioning: {max_val_positioning}\")\n",
    "        \n",
    "        for j in range(prediction_note_count):\n",
    "            curr_note_time = curr_time + j * 0.02\n",
    "            prediction_timing = prediction[j][0]\n",
    "            if prediction_timing < timing_threshhold:\n",
    "                continue\n",
    "            prediction_positioning = prediction[j][1:]\n",
    "            \n",
    "            # Place only 1 note per timing or to place many notes. More than 1 note can get super repetitive more easily, but both are of quite poor quality so far.\n",
    "            max_one_note_per_placement = True\n",
    "            if max_one_note_per_placement:\n",
    "                note_prediction_iter = np.argmax(prediction_positioning)\n",
    "                prediction_positioning_enumerated = [(note_prediction_iter, prediction_positioning[note_prediction_iter])]\n",
    "            else:\n",
    "                prediction_positioning_enumerated = [(i, note_prediction) for i, note_prediction in enumerate(prediction_positioning) if note_prediction > positioning_threshhold]\n",
    "\n",
    "            for note_prediction_iter, note_prediction in prediction_positioning_enumerated:\n",
    "                    line_layer = note_prediction_iter % 3\n",
    "                    line_index = int(note_prediction_iter / 3) % 4\n",
    "                    direction = int(note_prediction_iter / 12) % 9\n",
    "                    color = int(note_prediction_iter / 108)\n",
    "                    generated_notes.append(Note(curr_note_time, line_index, line_layer, color, direction))\n",
    "                    prev_note_segment[j][0] = 1\n",
    "                    prev_note_segment[j][1 + note_prediction_iter] = 1\n",
    "    \n",
    "    generated_notes.sort(key=lambda note: note.time)\n",
    "    return generated_notes\n",
    "\n",
    "intensity_timings_per_second = 9 # model input for number of correct timings per second\n",
    "base_intensity_notes_per_second = intensity_timings_per_second * 2 # model input for sum of '1's in the prediction segment. Increasing this value should result in more stacks and sliders.\n",
    "generated_notes = validate_model(song_data, segment_duration, timing_threshhold=0.35, positioning_threshhold=0.8, intensity_1=intensity_timings_per_second/25, intensity_2=base_intensity_notes_per_second/25)\n",
    "\n",
    "if len(generated_notes) > 0:\n",
    "    average_notes_per_second = len(generated_notes)/generated_notes[-1].time\n",
    "else:\n",
    "    average_notes_per_second = -1\n",
    "\n",
    "print(f\"Average nps: {average_notes_per_second}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\bs-map-generator\\\\src\\\\validation\\\\mazule.zip'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(base_validation_path + validation_map + \"/Info.dat\", \"rb\") as f:\n",
    "    info_json = json.load(f)\n",
    "    bpm = info_json[\"_beatsPerMinute\"]\n",
    "    \n",
    "with open(base_validation_path + validation_map + \"/ExpertStandard.dat\", \"rb\") as f:\n",
    "    diff_json = json.load(f)\n",
    "\n",
    "diff_json[\"_notes\"] = [{\"_time\": note.time / 60 * bpm, \"_lineIndex\": int(note.lineIndex), \"_lineLayer\": int(note.lineLayer), \"_type\": int(note.type), \"_cutDirection\": int(note.direction)} for note in generated_notes]\n",
    "with open(base_validation_path + validation_map + \"/ExpertStandard.dat\", \"w\") as f:\n",
    "    json.dump(diff_json, f)\n",
    "    \n",
    "shutil.make_archive(base_validation_path + validation_map, 'zip', base_validation_path + validation_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
