{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import random\n",
    "from data_loader import full_load_map, data_dir, load_map, Note\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import json\n",
    "import shutil\n",
    "import os\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.optimizer.set_jit('autoclustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1470258369\n",
    "\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator_multi_process(map_folders):\n",
    "    map_folders = [map_folder.decode('UTF-8') for map_folder in map_folders]\n",
    "    max_workers = 12\n",
    "    items_in_queue = max_workers * 3\n",
    "    queued_maps = items_in_queue\n",
    "    cancel = False\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        map_tasks = [(executor.submit(full_load_map, map_folder), map_folder) for map_folder in map_folders[:items_in_queue]]\n",
    "        while len(map_tasks) > 0:\n",
    "            map_task, map_folder = map_tasks.pop(0)\n",
    "            try:\n",
    "                if cancel:\n",
    "                    map_task.cancel()\n",
    "                    continue\n",
    "                results = map_task.result()\n",
    "                for result in results:\n",
    "                    x_context_prev_audio, x_context_prev_notes, x_context_audio, y_context_notes, z_timing_counts, z_note_counts, z_note_pos_counts, z_acc_prediction, z_speed_prediction = result\n",
    "                    yield (x_context_prev_audio), (x_context_prev_notes), (x_context_audio), z_timing_counts, z_note_counts/20, z_note_pos_counts/10, z_acc_prediction, z_speed_prediction, (y_context_notes)\n",
    "            except InterruptedError as ke:\n",
    "                cancel = True\n",
    "            except Exception as exc:\n",
    "                pass\n",
    "                # if str(exc) != \"'_version'\" and str(exc) != 'not v2':\n",
    "                #     print(map_folder)\n",
    "                #     print(exc)\n",
    "                #     traceback.print_exc()\n",
    "            finally:\n",
    "                if not cancel:\n",
    "                    queued_maps += 1\n",
    "                    if queued_maps < len(map_folders):\n",
    "                        map_tasks.append((executor.submit(full_load_map, map_folders[queued_maps]), map_folders[queued_maps]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ds_for_files(map_folders, batch_size, name, cache=False, shuffle=False):\n",
    "    ds = tf.data.Dataset.from_generator(data_generator_multi_process, args=[map_folders], output_signature=(\n",
    "        tf.TensorSpec(shape=(None, 2, 87, 129), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 2, 40, 25), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 2, 87, 129), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 1), dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 2, 40, 25), dtype=tf.float32),\n",
    "        # tf.TensorSpec(shape=(None, 1025, 44), dtype=tf.float32),\n",
    "        # tf.TensorSpec(shape=(None, 35), dtype=tf.float32),\n",
    "    ))\n",
    "    ds = ds.flat_map(lambda x1, x2, x3, x4, x5, x6, x7, x8, y: tf.data.Dataset.from_tensor_slices((x1, x2, x3, x4, x5, x6, x7, x8, y)))\n",
    "    # ds = ds.prefetch(20000)\n",
    "\n",
    "    if cache:\n",
    "        # ds = ds.cache()\n",
    "        ds = ds.cache(f\"./somethingsomething/{name}\")\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(5000, reshuffle_each_iteration=True)\n",
    "        # ds = ds.shuffle(len([v for v in ds]), reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(256)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I currently cache the entire dataset, since the data loading part is quite compute intensive. Added a limit of 50 maps to avoid running out of ram on a test run.\n",
    "maps = [path.replace(\"\\\\\", \"/\") for path in glob.glob(\"../data/maps/*\")]\n",
    "random.shuffle(maps)\n",
    "# maps = maps[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "val_split = 0.1\n",
    "train_ds = create_ds_for_files(maps[int(len(maps)*val_split):], batch_size, \"train\", False, True)\n",
    "val_ds = create_ds_for_files(maps[:int(len(maps)*val_split)], batch_size, \"val\", False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preload the dataset into cache to keep the data loading errors away from the training logs\n",
    "# discard_value = [0 for v in tqdm(train_ds)]\n",
    "\n",
    "# discard_value = [0 for v in tqdm(val_ds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_block():\n",
    "    input_audio = tf.keras.Input(shape=(87, 129, 1), dtype=\"float32\")\n",
    "    l = input_audio\n",
    "    l = tf.keras.layers.Conv2D(128, 5, activation=\"relu\", padding=\"same\")(l)\n",
    "    l = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding=\"same\")(l)\n",
    "    l = tf.keras.layers.Conv2D(128, 3, activation=\"relu\")(l)\n",
    "    l = tf.keras.layers.MaxPooling2D(pool_size=(1, 2))(l)\n",
    "    l = tf.keras.layers.Conv2D(128, 3, activation=\"relu\")(l)\n",
    "    l = tf.keras.layers.MaxPooling2D(pool_size=(1, 2))(l)\n",
    "    l = tf.keras.layers.Reshape((40, -1))(l)\n",
    "    l = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(128, activation=\"tanh\"))(l)\n",
    "    return tf.keras.Model(input_audio, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_block_stereo():\n",
    "    l_audio_block = audio_block()\n",
    "    \n",
    "    input_audio = tf.keras.Input(shape=(2, 87, 129, 1), dtype=\"float32\")\n",
    "    l = input_audio\n",
    "    l = tf.keras.layers.TimeDistributed(l_audio_block)(l)\n",
    "    l = tf.keras.layers.Permute((2, 1, 3))(l)\n",
    "    l = tf.keras.layers.Reshape((40, -1))(l)\n",
    "    l = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(l)\n",
    "    l = tf.keras.layers.LSTM(64, return_sequences=True)(l)\n",
    "    return tf.keras.Model(input_audio, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def note_positioning_block():\n",
    "    input_timings = tf.keras.Input(shape=(2, 40, 1), dtype=\"float32\")\n",
    "    input_features = tf.keras.Input(shape=(40, 128), dtype=\"float32\")\n",
    "    \n",
    "    l_timings = tf.keras.layers.Permute((2, 1, 3))(input_timings)\n",
    "    l_timings = tf.keras.layers.Reshape((40, -1))(l_timings)\n",
    "\n",
    "    l = tf.keras.layers.Concatenate(axis=2)([l_timings, input_features])\n",
    "    l = tf.keras.layers.LSTM(256, return_sequences=True)(l)\n",
    "    l = tf.keras.layers.LSTM(256, return_sequences=True)(l)\n",
    "    l_pos_out = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(24, activation=\"sigmoid\"))(l)\n",
    "    l = tf.keras.layers.Concatenate(axis=2)([l_pos_out, l])\n",
    "    l = tf.keras.layers.LSTM(128, return_sequences=True)(l)\n",
    "    l = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256, activation=\"relu\"))(l)\n",
    "    l_angle_out = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(24, activation=\"linear\"))(l)\n",
    "    \n",
    "    l_pos_out = tf.keras.layers.Reshape((40, 2, -1))(l_pos_out)\n",
    "    l_pos_out = tf.keras.layers.Permute((2, 1, 3))(l_pos_out)\n",
    "    l_angle_out = tf.keras.layers.Reshape((40, 2, -1))(l_angle_out)\n",
    "    l_angle_out = tf.keras.layers.Permute((2, 1, 3))(l_angle_out)\n",
    "    \n",
    "    return tf.keras.Model([input_timings, input_features], [l_pos_out, l_angle_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(audio_model):\n",
    "    input_prev_audio = tf.keras.Input(shape=(2, 87, 129, 1), dtype=\"float32\")\n",
    "    input_prev_notes = tf.keras.Input(shape=(2, 40, 25), dtype=\"float32\")\n",
    "    input_audio = tf.keras.Input(shape=(2, 87, 129, 1), dtype=\"float32\")\n",
    "    input_acc_prediction = tf.keras.Input(shape=(1), dtype=\"float32\")\n",
    "    input_speed_prediction = tf.keras.Input(shape=(1), dtype=\"float32\")\n",
    "\n",
    "    l_prev_audio = audio_model(input_prev_audio)\n",
    "    l_audio = audio_model(input_audio)\n",
    "    \n",
    "    l_prev_notes = tf.keras.layers.Permute((2, 1, 3))(input_prev_notes)\n",
    "    l_prev_notes = tf.keras.layers.Reshape((40, -1))(l_prev_notes)\n",
    "    \n",
    "    l_prev = tf.keras.layers.Concatenate(axis=2)([l_prev_audio, l_prev_notes])\n",
    "    l_prev = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(l_prev)\n",
    "    l_prev = tf.keras.layers.LSTM(64)(l_prev)\n",
    "    \n",
    "    l_input_acc_prediction = tf.keras.layers.RepeatVector(40)(input_acc_prediction)\n",
    "    l_input_speed_prediction = tf.keras.layers.RepeatVector(40)(input_speed_prediction)\n",
    "    l_prev = tf.keras.layers.RepeatVector(40)(l_prev)\n",
    "    l = tf.keras.layers.Concatenate(axis=2)([l_audio, l_prev, l_input_acc_prediction, l_input_speed_prediction])\n",
    "    l = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(l)\n",
    "    l = tf.keras.layers.LSTM(128, return_sequences=True)(l)\n",
    "    l_timings_out = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(2, activation=\"sigmoid\"))(l)\n",
    "    l_timings_out = tf.keras.layers.Permute((2, 1))(l_timings_out)\n",
    "    l_timings_out = tf.keras.layers.Reshape((2, 40, -1))(l_timings_out)\n",
    "\n",
    "    note_positioning_l = note_positioning_block()\n",
    "    \n",
    "    l_pos_out, l_angle_out = note_positioning_l([l_timings_out, l])\n",
    "\n",
    "    model = tf.keras.Model(inputs = [input_prev_audio, input_prev_notes, input_audio, input_acc_prediction, input_speed_prediction], outputs = [l_timings_out, l_pos_out, l_angle_out])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model(audio_model):\n",
    "    input_prev_audio = tf.keras.Input(shape=(2, 87, 129, 1), dtype=\"float32\")\n",
    "    input_audio = tf.keras.Input(shape=(2, 87, 129, 1), dtype=\"float32\")\n",
    "\n",
    "    input_prev_notes = tf.keras.Input(shape=(2, 40, 25), dtype=\"float32\")\n",
    "    input_notes = tf.keras.Input(shape=(2, 40, 25), dtype=\"float32\")\n",
    "\n",
    "    input_acc_prediction = tf.keras.Input(shape=(1), dtype=\"float32\")\n",
    "    input_speed_prediction = tf.keras.Input(shape=(1), dtype=\"float32\")\n",
    "    \n",
    "    l_acc_prediction = tf.keras.layers.RepeatVector(80)(input_acc_prediction)\n",
    "    l_speed_prediction = tf.keras.layers.RepeatVector(80)(input_speed_prediction)\n",
    "    \n",
    "    l_prev_audio = audio_model(input_prev_audio)\n",
    "    l_audio = audio_model(input_audio)\n",
    "\n",
    "    l_audio = tf.keras.layers.Concatenate(axis=1)([l_prev_audio, l_audio])\n",
    "    l_notes = tf.keras.layers.Concatenate(axis=2)([input_prev_notes, input_notes])\n",
    "    \n",
    "    l_notes = tf.keras.layers.Permute((2, 1, 3))(l_notes)\n",
    "    l_notes = tf.keras.layers.Reshape((80, -1))(l_notes)\n",
    "    \n",
    "    l = tf.keras.layers.Concatenate(axis=2)([l_audio, l_notes, l_acc_prediction, l_speed_prediction])\n",
    "    \n",
    "    l = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(l)\n",
    "    l = tf.keras.layers.LSTM(256, return_sequences=True)(l)\n",
    "    l = tf.keras.layers.LSTM(256)(l)\n",
    "    l = tf.keras.layers.Dense(1, activation=\"sigmoid\")(l)\n",
    "    \n",
    "    return tf.keras.Model([input_prev_audio, input_audio, input_prev_notes, input_notes, input_acc_prediction, input_speed_prediction], [l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_model = audio_block_stereo()\n",
    "\n",
    "model_generator = make_model(audio_model)\n",
    "model_discriminator = make_discriminator_model(audio_model)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize the model\n",
    "# tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_train_d_loss = tf.keras.metrics.Mean(name='metric_train_d_loss')\n",
    "metric_train_g_loss = tf.keras.metrics.Mean(name='metric_train_g_loss')\n",
    "metric_train_simple_loss = tf.keras.metrics.Mean(name='metric_train_simple_loss')\n",
    "\n",
    "metric_train_timing_loss = tf.keras.metrics.Mean(name='metric_train_timing_loss')\n",
    "metric_train_pos_loss = tf.keras.metrics.Mean(name='metric_train_pos_loss')\n",
    "metric_train_angle_loss = tf.keras.metrics.Mean(name='metric_train_angle_loss')\n",
    "metric_train_loss = tf.keras.metrics.Mean(name='metric_train_loss')\n",
    "metric_train_discriminator_loss = tf.keras.metrics.Mean(name='metric_train_discriminator_loss')\n",
    "metric_val_timing_loss = tf.keras.metrics.Mean(name='metric_val_timing_loss')\n",
    "metric_val_pos_loss = tf.keras.metrics.Mean(name='metric_val_pos_loss')\n",
    "metric_val_angle_loss = tf.keras.metrics.Mean(name='metric_val_angle_loss')\n",
    "metric_val_loss = tf.keras.metrics.Mean(name='metric_val_loss')\n",
    "metric_val_discriminator_loss = tf.keras.metrics.Mean(name='metric_val_discriminator_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom loss\n",
    "Calculates 2 separate values:\n",
    " - timing loss - simple loss based on when the notes were placed\n",
    " - positioning loss - loss based on the position and direction of the placed note, adjusted for the number of notes that appear in different positions to avoid a massive bias towards placing most commonly appearing notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def custom_loss(y, predictions):\n",
    "    timing_predictions, pos_predictions, angle_predictions = predictions\n",
    "    org_timing_loss_matrix = tf.square(y[:, :, :, :1] - timing_predictions) * (y[:, :, :, :1] + 0.08)\n",
    "    timing_loss_matrix = org_timing_loss_matrix\n",
    "    timing_loss = tf.reduce_mean(timing_loss_matrix) * 10\n",
    "    \n",
    "    # positioning_loss_matrix = tf.square(y[:, :, 1:] - positioning_predictions) * (y[:, :, :1]) * (y[:, :, 1:] * note_poss_loss_balance + 0.0169)\n",
    "    # positioning_loss = tf.reduce_sum(positioning_loss_matrix) / tf.reduce_sum(y[:, :, 1:]) * 0.5\n",
    "    \n",
    "    y_positioning_loss_matrix = tf.square(y[:, :, :, 1::2] - pos_predictions) * (y[:, :, :, :1]) * (y[:, :, :, 1::2] + 0.069)\n",
    "    y_positioning_loss = tf.reduce_sum(y_positioning_loss_matrix) / tf.reduce_sum(y[:, :, :, 1::2]) * 0.5\n",
    "    \n",
    "    y_positioning_angle_loss_matrix = tf.square(tf.minimum(tf.abs(y[:, :, :, 2::2] - angle_predictions), tf.minimum(tf.abs(y[:, :, :, 2::2] - angle_predictions - 1), tf.abs(y[:, :, :, 2::2] - angle_predictions + 1)))) * (y[:, :, :, 1::2])\n",
    "    y_positioning_angle_loss = tf.reduce_sum(y_positioning_angle_loss_matrix) / tf.reduce_sum(y[:, :, :, 1::2]) * 20\n",
    "    \n",
    "    loss = timing_loss + y_positioning_loss + y_positioning_angle_loss\n",
    "\n",
    "    return timing_loss, y_positioning_loss, y_positioning_angle_loss, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(optimizer, data):\n",
    "    x1, x2, x3, x4, x5, x6, x7, x8, y = data\n",
    "    timing_predictions, pos_predictions, angle_predictions = model_generator([x1, x2, x3, x7, x8], training=True)\n",
    "    \n",
    "    generator_output = tf.concat([timing_predictions, pos_predictions, angle_predictions], axis=3)\n",
    "    reshaped_x2 = tf.concat([x2[:, :, :, :1], x2[:, :, :, 1::2], x2[:, :, :, 2::2]], axis=3)\n",
    "    reshaped_y = tf.concat([y[:, :, :, :1], y[:, :, :, 1::2], y[:, :, :, 2::2]], axis=3)\n",
    "    labels = tf.concat([tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0)\n",
    "    labels += 0.05 * tf.random.uniform(labels.shape)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model_discriminator([tf.concat([x1, x1], axis=0), tf.concat([x3, x3], axis=0), tf.concat([reshaped_x2, reshaped_x2], axis=0), tf.concat([generator_output, reshaped_y], axis=0), tf.concat([x7, x7], axis=0), tf.concat([x8, x8], axis=0)])\n",
    "        d_loss = loss_fn(labels, predictions)\n",
    "\n",
    "    grads = tape.gradient(d_loss, model_discriminator.trainable_weights)\n",
    "    discriminator_optimizer.apply_gradients(zip(grads, model_discriminator.trainable_weights))\n",
    "    \n",
    "    misleading_labels = tf.zeros((batch_size, 1))\n",
    "    with tf.GradientTape() as tape:\n",
    "        timing_predictions, pos_predictions, angle_predictions = model_generator([x1, x2, x3, x7, x8], training=True)\n",
    "        generator_output = tf.concat([timing_predictions, pos_predictions, angle_predictions], axis=3)\n",
    "        predictions = model_discriminator([x1, x3, reshaped_x2, generator_output, x7, x8])\n",
    "        g_loss = loss_fn(misleading_labels, predictions)\n",
    "        \n",
    "    grads = tape.gradient(g_loss, model_generator.trainable_weights)\n",
    "    generator_optimizer.apply_gradients(zip(grads, model_generator.trainable_weights))\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        timing_predictions, pos_predictions, angle_predictions = model_generator([x1, x2, x3, x7, x8], training=True)\n",
    "        timing_loss, y_positioning_loss, y_positioning_angle_loss, simple_loss = custom_loss(y, (timing_predictions, pos_predictions, angle_predictions))\n",
    "        \n",
    "    grads_simple = tape.gradient(simple_loss, model_generator.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads_simple, model_generator.trainable_weights))\n",
    "    \n",
    "    metric_train_d_loss(d_loss)\n",
    "    metric_train_g_loss(g_loss)\n",
    "    metric_train_simple_loss(simple_loss)\n",
    "\n",
    "# @tf.function\n",
    "# def val_step(data):\n",
    "#     x1, x2, x3, x4, x5, x6, x7, x8, y = data\n",
    "    \n",
    "#     predictions = model([x1, x2, x3, x7, x8], training=False)\n",
    "#     # with tf.device('/CPU:0'):\n",
    "#     timing_loss, positioning_loss, angle_loss, loss = custom_loss(y, predictions)\n",
    "        \n",
    "#     metric_train_d_loss(d_loss)\n",
    "#     metric_train_g_loss(g_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results validation\n",
    "\n",
    "- specify the correct folder with maps for which you would want to generate the map\n",
    "- add maps that you want to use for testing, better to avoid using the maps that already exist in the training dataset to avoid false positives of AI learning a specific map\n",
    "- add an Expert diff if doesn't exist, currently hardcoded to just override the Expert diff to avoid setting up all the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_validation(timing_threshhold, epoch, acc_prediction, speed_prediction):\n",
    "    base_validation_path = \"./validation\"\n",
    "    os.makedirs(base_validation_path, exist_ok=True)\n",
    "    \n",
    "    validation_map = \"DA42AF71F4CA5AD280C3F69BCA0BD6C6D1CDA06E\"\n",
    "    validation_map_dir = f\"{base_validation_path}/{validation_map}\"\n",
    "    (song_data, segment_duration), diffs = load_map(validation_map_dir)\n",
    "    \n",
    "    \n",
    "    angle_to_direction = {\n",
    "        180:0,\n",
    "        0:1,\n",
    "        90:2,\n",
    "        270:3,\n",
    "        135:4,\n",
    "        225:5,\n",
    "        45:6,\n",
    "        315:7\n",
    "    }\n",
    "    direction_to_angle = {\n",
    "        0: 180,\n",
    "        1: 0,\n",
    "        2: 90,\n",
    "        3: 270,\n",
    "        4: 135,\n",
    "        5: 225,\n",
    "        6: 45,\n",
    "        7: 315\n",
    "    }\n",
    "\n",
    "\n",
    "    def get_note_angle(direction):\n",
    "        return direction_to_angle[direction] / 360\n",
    "    \n",
    "    def get_note_direction(angle):\n",
    "        angle = int(angle * 360)\n",
    "        if angle % 45 > 22.5:\n",
    "            angle += 45\n",
    "        angle = (angle - angle % 45) % 360\n",
    "        return angle_to_direction[angle]\n",
    "\n",
    "    def validate_model(song_data, segment_duration, timing_threshhold, positioning_threshhold, intensity_1, intensity_2, acc_prediction, speed_prediction, note_pos_count):\n",
    "        context_length = 1\n",
    "        prediction_note_count = context_length * 40\n",
    "        prediction_note_time_length = context_length / prediction_note_count\n",
    "\n",
    "        context_steps = int(context_length / segment_duration) + 1\n",
    "        step_size = context_steps\n",
    "        \n",
    "        generated_notes = []\n",
    "        max_val_timing = 0\n",
    "        max_val_positioning = 0\n",
    "        \n",
    "        zero_notes = 1\n",
    "        one_notes = 1\n",
    "        \n",
    "        prev_note_segment = ([[0]*25 for i in range(prediction_note_count)], [[0]*25 for i in range(prediction_note_count)])\n",
    "        prev_audio_segment = song_data[:, :context_steps, :]\n",
    "        with tqdm(range(context_steps, song_data.shape[1] - context_steps, step_size)) as _tqdm:\n",
    "          for i in _tqdm:\n",
    "            curr_time = i * segment_duration\n",
    "            \n",
    "            x_context_prev_audio = prev_audio_segment\n",
    "            x_context_prev_notes = prev_note_segment\n",
    "            x_context_audio = song_data[:, i:i+context_steps, :]\n",
    "            timing_prediction, placement_prediction, placement_angle_prediction = model_generator([np.array([x_context_prev_audio]), np.array([x_context_prev_notes]), np.array([x_context_audio]), np.array([acc_prediction + (random.random() - 0.5) * 0.1]), np.array([speed_prediction + (random.random() - 0.5) * 0.1])], training=False)\n",
    "            timing_prediction = tf.where(timing_prediction > timing_threshhold, 1, 0)\n",
    "            timing_prediction = np.array(timing_prediction[0])\n",
    "            placement_prediction = np.array(placement_prediction[0])\n",
    "            placement_angle_prediction = np.array(placement_angle_prediction[0])\n",
    "            \n",
    "            \n",
    "            x_context_prev_audio = x_context_audio\n",
    "            prev_note_segment = ([[0]*25 for i in range(prediction_note_count)], [[0]*25 for i in range(prediction_note_count)])\n",
    "\n",
    "            # I use them to find values that would generate a reasonable number of notes.\n",
    "            # Small adjustments to the model and it's loss function can significantly shift the actual number values.\n",
    "            # if max_val_timing < np.max(timing_prediction):\n",
    "            #     max_val_timing = np.max(timing_prediction)\n",
    "            #     print(f\"max_timing: {max_val_timing}\")\n",
    "                \n",
    "                # if max_val_positioning < np.max(positioning_prediction):\n",
    "                #     max_val_positioning = np.max(positioning_prediction)\n",
    "                #     print(f\"max_positioning: {max_val_positioning}\")\n",
    "            \n",
    "            for j in range(prediction_note_count):\n",
    "                for color in range(2):\n",
    "                    curr_note_time = curr_time + j * prediction_note_time_length\n",
    "                    prediction_timing = timing_prediction[color][j][0]\n",
    "                    if prediction_timing < timing_threshhold:\n",
    "                        continue\n",
    "                    prediction_positioning = placement_prediction[color][j]\n",
    "                    prediction_angle = placement_angle_prediction[color][j]\n",
    "                    \n",
    "                    # prediction_positioning[:12] = prediction_positioning[:12] * (one_notes / (zero_notes + one_notes + 0.00001))\n",
    "                    # prediction_positioning[12:] = prediction_positioning[12:] * (zero_notes / (zero_notes + one_notes + 0.00001))\n",
    "\n",
    "                    # Place only 1 note per timing or to place many notes. More than 1 note can get super repetitive more easily, but both are of quite poor quality so far.\n",
    "                    max_one_note_per_placement = True\n",
    "                    if max_one_note_per_placement:\n",
    "                        note_prediction_iter = np.argmax(prediction_positioning)\n",
    "                        prediction_positioning_enumerated = [(note_prediction_iter, prediction_positioning[note_prediction_iter], prediction_angle[note_prediction_iter])]\n",
    "                    else:\n",
    "                        prediction_positioning_enumerated = [(i, note_prediction, prediction_angle[i]) for i, note_prediction in enumerate(prediction_positioning) if note_prediction > positioning_threshhold]\n",
    "\n",
    "                    for note_prediction_iter, note_prediction, angle_prediction in prediction_positioning_enumerated:\n",
    "                            line_layer = note_prediction_iter % 3\n",
    "                            line_index = int(note_prediction_iter / 3) % 4\n",
    "                            direction = get_note_direction(angle_prediction)\n",
    "                            generated_notes.append(Note(curr_note_time, line_index, line_layer, color, direction))\n",
    "                            if color == 0:\n",
    "                                zero_notes += 1\n",
    "                            else:\n",
    "                                one_notes += 1\n",
    "                            prev_note_segment[color][j][0] = 1\n",
    "                            prev_note_segment[color][j][1 + note_prediction_iter * 2] = 1\n",
    "                            prev_note_segment[color][j][1 + note_prediction_iter * 2 + 1] = get_note_angle(direction)\n",
    "            if len(generated_notes) > 0:\n",
    "                average_notes_per_second = len(generated_notes)/generated_notes[-1].time\n",
    "            else:\n",
    "                average_notes_per_second = -1\n",
    "            _tqdm.set_postfix(average_notes_per_second=average_notes_per_second)\n",
    "        \n",
    "        generated_notes.sort(key=lambda note: note.time)\n",
    "        return generated_notes\n",
    "\n",
    "    intensity_timings_per_second = 7 # model input for number of correct timings per second\n",
    "    intensity_notes_per_second = intensity_timings_per_second # model input for sum of '1's in the prediction segment. Increasing this value should result in more stacks and sliders.\n",
    "    note_pos_count = 3\n",
    "    generated_notes = validate_model(song_data, segment_duration, timing_threshhold=timing_threshhold, positioning_threshhold=0.45, intensity_1=intensity_timings_per_second, intensity_2=intensity_notes_per_second/20, acc_prediction=acc_prediction, speed_prediction=speed_prediction, note_pos_count=note_pos_count/10)\n",
    "\n",
    "    if len(generated_notes) > 0:\n",
    "        average_notes_per_second = len(generated_notes)/generated_notes[-1].time\n",
    "    else:\n",
    "        average_notes_per_second = -1\n",
    "    \n",
    "    with open(validation_map_dir + \"/Info.dat\", \"rb\") as f:\n",
    "        info_json = json.load(f)\n",
    "        bpm = info_json[\"_beatsPerMinute\"]\n",
    "        \n",
    "    with open(validation_map_dir + \"/ExpertStandard.dat\", \"rb\") as f:\n",
    "        diff_json = json.load(f)\n",
    "\n",
    "    diff_json[\"_notes\"] = [{\"_time\": note.time / 60 * bpm, \"_lineIndex\": int(note.lineIndex), \"_lineLayer\": int(note.lineLayer), \"_type\": int(note.type), \"_cutDirection\": int(note.direction)} for note in generated_notes]\n",
    "    if len(diff_json[\"_notes\"]) == 0:\n",
    "        diff_json[\"_notes\"] = [{\"_time\": 1, \"_lineIndex\": 0, \"_lineLayer\": 0, \"_cutDirection\": 0, \"_type\": 0}]\n",
    "    with open(validation_map_dir + \"/ExpertStandard.dat\", \"w\") as f:\n",
    "        json.dump(diff_json, f)\n",
    "        \n",
    "    shutil.make_archive(f\"{validation_map_dir}q{epoch}q{timing_threshhold}q{average_notes_per_second}q{acc_prediction}q{speed_prediction}\", 'zip', validation_map_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch train: 0: : 0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch train: 0: : 13096batch [32:50,  7.20batch/s, d_loss=0.661, g_loss=0.912, simple_loss=2.79]"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, 15):\n",
    "    metric_train_d_loss.reset_states()\n",
    "    metric_train_g_loss.reset_states()\n",
    "    metric_train_timing_loss.reset_states()\n",
    "    metric_train_pos_loss.reset_states()\n",
    "    metric_train_angle_loss.reset_states()\n",
    "    metric_train_loss.reset_states()\n",
    "    metric_val_timing_loss.reset_states()\n",
    "    metric_val_pos_loss.reset_states()\n",
    "    metric_val_angle_loss.reset_states()\n",
    "    metric_val_loss.reset_states()\n",
    "\n",
    "    if epoch <= 0:\n",
    "        generator_optimizer.learning_rate.assign(0.0001)\n",
    "        discriminator_optimizer.learning_rate.assign(0.00002)\n",
    "        optimizer.learning_rate.assign(0.00002)\n",
    "    elif epoch <= 2:\n",
    "        generator_optimizer.learning_rate.assign(0.00005)\n",
    "        discriminator_optimizer.learning_rate.assign(0.00001)\n",
    "        optimizer.learning_rate.assign(0.00001)\n",
    "    elif epoch <= 4:\n",
    "        generator_optimizer.learning_rate.assign(0.000025)\n",
    "        discriminator_optimizer.learning_rate.assign(0.000005)\n",
    "        optimizer.learning_rate.assign(0.000005)\n",
    "    elif epoch <= 6:\n",
    "        generator_optimizer.learning_rate.assign(0.000025)\n",
    "        discriminator_optimizer.learning_rate.assign(0.0000025)\n",
    "        optimizer.learning_rate.assign(0.0000025)\n",
    "    elif epoch <= 8:\n",
    "        generator_optimizer.learning_rate.assign(0.00001)\n",
    "        discriminator_optimizer.learning_rate.assign(0.00001)\n",
    "        optimizer.learning_rate.assign(0.00001)\n",
    "    elif epoch <= 10:\n",
    "        generator_optimizer.learning_rate.assign(0.0000025)\n",
    "        discriminator_optimizer.learning_rate.assign(0.0000025)\n",
    "        optimizer.learning_rate.assign(0.0000025)\n",
    "    \n",
    "    with tqdm(train_ds.enumerate(), unit=\"batch\") as _tqdm:\n",
    "        _tqdm.set_description(f\"Epoch train: {epoch}\")\n",
    "        for step, data in _tqdm:\n",
    "            train_step(optimizer, data)\n",
    "            _tqdm.set_postfix(\n",
    "                d_loss=metric_train_d_loss.result().numpy(),\n",
    "                g_loss=metric_train_g_loss.result().numpy(),\n",
    "                simple_loss=metric_train_simple_loss.result().numpy(),\n",
    "            )\n",
    "    \n",
    "    # if 'val_ds' in locals() or 'val_ds' in globals():\n",
    "    #     with tqdm(val_ds.enumerate(), unit=\"batch\") as _tqdm:\n",
    "    #         _tqdm.set_description(f\"Epoch val: {epoch}\")\n",
    "    #         for step, data in _tqdm:\n",
    "    #             val_step(model, data)\n",
    "    #             _tqdm.set_postfix(\n",
    "    #                 timing_loss=metric_val_timing_loss.result().numpy(),\n",
    "    #                 pos_loss=metric_val_pos_loss.result().numpy(),\n",
    "    #                 angle_loss=metric_val_angle_loss.result().numpy(),\n",
    "    #                 loss=metric_val_loss.result().numpy(),\n",
    "    #             )\n",
    "    \n",
    "    full_validation(0.825 + (random.random() - 0.5)*0.2, epoch, 0.775 + (random.random() - 0.5)*0.25, 0.35 + (random.random() - 0.5)*0.25)\n",
    "    full_validation(0.825 + (random.random() - 0.5)*0.2, epoch, 0.775 + (random.random() - 0.5)*0.25, 0.35 + (random.random() - 0.5)*0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:17<00:00,  9.40it/s, average_notes_per_second=1.98]\n"
     ]
    }
   ],
   "source": [
    "full_validation(0.1 + (random.random() - 0.5)*0.1, epoch, 0.8 + (random.random() - 0.5)*0.15, 0.3 + (random.random() - 0.5)*0.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
